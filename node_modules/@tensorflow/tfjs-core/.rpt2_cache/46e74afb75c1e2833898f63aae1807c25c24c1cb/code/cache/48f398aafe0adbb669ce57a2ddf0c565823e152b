{"code":"import * as tslib_1 from \"tslib\";\r\nimport { ENV } from '../environment';\r\nimport { keep, tidy } from '../globals';\r\nimport { scalar, zerosLike } from '../ops/ops';\r\nimport { SerializationMap } from '../serialization';\r\nimport { Optimizer } from './optimizer';\r\nvar AdadeltaOptimizer = (function (_super) {\r\n    tslib_1.__extends(AdadeltaOptimizer, _super);\r\n    function AdadeltaOptimizer(learningRate, rho, epsilon) {\r\n        if (epsilon === void 0) { epsilon = 1e-8; }\r\n        var _this = _super.call(this) || this;\r\n        _this.learningRate = learningRate;\r\n        _this.rho = rho;\r\n        _this.epsilon = epsilon;\r\n        _this.accumulatedGrads = {};\r\n        _this.accumulatedUpdates = {};\r\n        _this.c = keep(scalar(-learningRate));\r\n        _this.epsilonScalar = keep(scalar(epsilon));\r\n        _this.rhoScalar = keep(scalar(rho));\r\n        _this.oneMinusRho = keep(scalar(1 - rho));\r\n        return _this;\r\n    }\r\n    AdadeltaOptimizer.prototype.applyGradients = function (variableGradients) {\r\n        var _this = this;\r\n        var _loop_1 = function (variableName) {\r\n            var value = ENV.engine.registeredVariables[variableName];\r\n            if (this_1.accumulatedGrads[variableName] == null) {\r\n                var trainable_1 = false;\r\n                tidy(function () {\r\n                    _this.accumulatedGrads[variableName] =\r\n                        zerosLike(value).variable(trainable_1);\r\n                });\r\n            }\r\n            if (this_1.accumulatedUpdates[variableName] == null) {\r\n                var trainable_2 = false;\r\n                tidy(function () {\r\n                    _this.accumulatedUpdates[variableName] =\r\n                        zerosLike(value).variable(trainable_2);\r\n                });\r\n            }\r\n            var gradient = variableGradients[variableName];\r\n            var accumulatedGrad = this_1.accumulatedGrads[variableName];\r\n            var accumulatedUpdate = this_1.accumulatedUpdates[variableName];\r\n            tidy(function () {\r\n                var newAccumulatedGrad = _this.rhoScalar.mul(accumulatedGrad)\r\n                    .add(_this.oneMinusRho.mul(gradient.square()));\r\n                var updates = accumulatedUpdate.add(_this.epsilonScalar)\r\n                    .sqrt()\r\n                    .div(accumulatedGrad.add(_this.epsilonScalar).sqrt())\r\n                    .mul(gradient);\r\n                var newAccumulatedUpdate = _this.rhoScalar.mul(accumulatedUpdate)\r\n                    .add(_this.oneMinusRho.mul(updates.square()));\r\n                _this.accumulatedGrads[variableName].assign(newAccumulatedGrad);\r\n                _this.accumulatedUpdates[variableName].assign(newAccumulatedUpdate);\r\n                var newValue = _this.c.mul(updates).add(value);\r\n                value.assign(newValue);\r\n            });\r\n        };\r\n        var this_1 = this;\r\n        for (var variableName in variableGradients) {\r\n            _loop_1(variableName);\r\n        }\r\n    };\r\n    AdadeltaOptimizer.prototype.dispose = function () {\r\n        var _this = this;\r\n        this.c.dispose();\r\n        this.epsilonScalar.dispose();\r\n        this.rhoScalar.dispose();\r\n        this.oneMinusRho.dispose();\r\n        if (this.accumulatedUpdates != null) {\r\n            Object.keys(this.accumulatedUpdates)\r\n                .forEach(function (name) { return _this.accumulatedUpdates[name].dispose(); });\r\n            Object.keys(this.accumulatedGrads)\r\n                .forEach(function (name) { return _this.accumulatedGrads[name].dispose(); });\r\n        }\r\n    };\r\n    AdadeltaOptimizer.prototype.getConfig = function () {\r\n        return {\r\n            learningRate: this.learningRate,\r\n            rho: this.rho,\r\n            epsilon: this.epsilon\r\n        };\r\n    };\r\n    AdadeltaOptimizer.fromConfig = function (cls, config) {\r\n        return new cls(config.learningRate, config.rho, config.epsilon);\r\n    };\r\n    AdadeltaOptimizer.className = 'AdadeltaOptimizer';\r\n    return AdadeltaOptimizer;\r\n}(Optimizer));\r\nexport { AdadeltaOptimizer };\r\nSerializationMap.register(AdadeltaOptimizer);\r\n//# sourceMappingURL=adadelta_optimizer.js.map","map":"{\"version\":3,\"file\":\"adadelta_optimizer.js\",\"sourceRoot\":\"\",\"sources\":[\"../src/optimizers/adadelta_optimizer.ts\"],\"names\":[],\"mappings\":\";AAiBA,OAAO,EAAC,GAAG,EAAC,MAAM,gBAAgB,CAAC;AACnC,OAAO,EAAC,IAAI,EAAE,IAAI,EAAC,MAAM,YAAY,CAAC;AACtC,OAAO,EAAC,MAAM,EAAE,SAAS,EAAC,MAAM,YAAY,CAAC;AAE7C,OAAO,EAAoD,gBAAgB,EAAC,MAAM,kBAAkB,CAAC;AAIrG,OAAO,EAAC,SAAS,EAAC,MAAM,aAAa,CAAC;AAGtC;IAAuC,6CAAS;IAU9C,2BACc,YAAoB,EAAY,GAAW,EAC3C,OAAc;QAAd,wBAAA,EAAA,cAAc;QAF5B,YAGE,iBAAO,SAKR;QAPa,kBAAY,GAAZ,YAAY,CAAQ;QAAY,SAAG,GAAH,GAAG,CAAQ;QAC3C,aAAO,GAAP,OAAO,CAAO;QALpB,sBAAgB,GAAqB,EAAE,CAAC;QACxC,wBAAkB,GAAqB,EAAE,CAAC;QAMhD,KAAI,CAAC,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC;QACrC,KAAI,CAAC,aAAa,GAAG,IAAI,CAAC,MAAM,CAAC,OAAO,CAAC,CAAC,CAAC;QAC3C,KAAI,CAAC,SAAS,GAAG,IAAI,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC;QACnC,KAAI,CAAC,WAAW,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,GAAG,GAAG,CAAC,CAAC,CAAC;;IAC3C,CAAC;IAED,0CAAc,GAAd,UAAe,iBAAmC;QAAlD,iBA2CC;gCA1CY,YAAY;YACrB,IAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,mBAAmB,CAAC,YAAY,CAAC,CAAC;YAC3D,EAAE,CAAC,CAAC,OAAK,gBAAgB,CAAC,YAAY,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;gBAChD,IAAM,WAAS,GAAG,KAAK,CAAC;gBACxB,IAAI,CAAC;oBACH,KAAI,CAAC,gBAAgB,CAAC,YAAY,CAAC;wBAC/B,SAAS,CAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,WAAS,CAAC,CAAC;gBAC3C,CAAC,CAAC,CAAC;YACL,CAAC;YACD,EAAE,CAAC,CAAC,OAAK,kBAAkB,CAAC,YAAY,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;gBAClD,IAAM,WAAS,GAAG,KAAK,CAAC;gBACxB,IAAI,CAAC;oBACH,KAAI,CAAC,kBAAkB,CAAC,YAAY,CAAC;wBACjC,SAAS,CAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,WAAS,CAAC,CAAC;gBAC3C,CAAC,CAAC,CAAC;YACL,CAAC;YAED,IAAM,QAAQ,GAAG,iBAAiB,CAAC,YAAY,CAAC,CAAC;YACjD,IAAM,eAAe,GAAG,OAAK,gBAAgB,CAAC,YAAY,CAAC,CAAC;YAC5D,IAAM,iBAAiB,GAAG,OAAK,kBAAkB,CAAC,YAAY,CAAC,CAAC;YAEhE,IAAI,CAAC;gBACH,IAAM,kBAAkB,GACpB,KAAI,CAAC,SAAS,CAAC,GAAG,CAAC,eAAe,CAAC;qBAC9B,GAAG,CAAC,KAAI,CAAC,WAAW,CAAC,GAAG,CAAC,QAAQ,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC;gBAEtD,IAAM,OAAO,GAAG,iBAAiB,CAAC,GAAG,CAAC,KAAI,CAAC,aAAa,CAAC;qBACpC,IAAI,EAAE;qBACN,GAAG,CAAC,eAAe,CAAC,GAAG,CAAC,KAAI,CAAC,aAAa,CAAC,CAAC,IAAI,EAAE,CAAC;qBACnD,GAAG,CAAC,QAAQ,CAAC,CAAC;gBAEnC,IAAM,oBAAoB,GACtB,KAAI,CAAC,SAAS,CAAC,GAAG,CAAC,iBAAiB,CAAC;qBAChC,GAAG,CAAC,KAAI,CAAC,WAAW,CAAC,GAAG,CAAC,OAAO,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC;gBAErD,KAAI,CAAC,gBAAgB,CAAC,YAAY,CAAC,CAAC,MAAM,CAAC,kBAAkB,CAAC,CAAC;gBAC/D,KAAI,CAAC,kBAAkB,CAAC,YAAY,CAAC,CAAC,MAAM,CAAC,oBAAoB,CAAC,CAAC;gBAEnE,IAAM,QAAQ,GAAG,KAAI,CAAC,CAAC,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,CAAC;gBAChD,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;YACzB,CAAC,CAAC,CAAC;QACL,CAAC;;QAzCD,GAAG,CAAC,CAAC,IAAM,YAAY,IAAI,iBAAiB,CAAC;oBAAlC,YAAY;SAyCtB;IACH,CAAC;IAED,mCAAO,GAAP;QAAA,iBAWC;QAVC,IAAI,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC;QACjB,IAAI,CAAC,aAAa,CAAC,OAAO,EAAE,CAAC;QAC7B,IAAI,CAAC,SAAS,CAAC,OAAO,EAAE,CAAC;QACzB,IAAI,CAAC,WAAW,CAAC,OAAO,EAAE,CAAC;QAC3B,EAAE,CAAC,CAAC,IAAI,CAAC,kBAAkB,IAAI,IAAI,CAAC,CAAC,CAAC;YACpC,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,kBAAkB,CAAC;iBAC/B,OAAO,CAAC,UAAA,IAAI,IAAI,OAAA,KAAI,CAAC,kBAAkB,CAAC,IAAI,CAAC,CAAC,OAAO,EAAE,EAAvC,CAAuC,CAAC,CAAC;YAC9D,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,gBAAgB,CAAC;iBAC7B,OAAO,CAAC,UAAA,IAAI,IAAI,OAAA,KAAI,CAAC,gBAAgB,CAAC,IAAI,CAAC,CAAC,OAAO,EAAE,EAArC,CAAqC,CAAC,CAAC;QAC9D,CAAC;IACH,CAAC;IACD,qCAAS,GAAT;QACE,MAAM,CAAC;YACL,YAAY,EAAE,IAAI,CAAC,YAAY;YAC/B,GAAG,EAAE,IAAI,CAAC,GAAG;YACb,OAAO,EAAE,IAAI,CAAC,OAAO;SACtB,CAAC;IACJ,CAAC;IACM,4BAAU,GAAjB,UACI,GAA+B,EAAE,MAAkB;QACrD,MAAM,CAAC,IAAI,GAAG,CAAC,MAAM,CAAC,YAAY,EAAE,MAAM,CAAC,GAAG,EAAE,MAAM,CAAC,OAAO,CAAC,CAAC;IAClE,CAAC;IAtFM,2BAAS,GAAG,mBAAmB,CAAC;IAuFzC,wBAAC;CAAA,AAxFD,CAAuC,SAAS,GAwF/C;SAxFY,iBAAiB;AAyF9B,gBAAgB,CAAC,QAAQ,CAAC,iBAAiB,CAAC,CAAC\"}","dts":{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/rollup/tfjs-core/optimizers/adadelta_optimizer.d.ts","text":"import { ConfigDict, Serializable, SerializableConstructor } from '../serialization';\r\nimport { NamedVariableMap } from '../types';\r\nimport { Optimizer } from './optimizer';\r\nexport declare class AdadeltaOptimizer extends Optimizer {\r\n    protected learningRate: number;\r\n    protected rho: number;\r\n    protected epsilon: number;\r\n    static className: string;\r\n    private c;\r\n    private epsilonScalar;\r\n    private rhoScalar;\r\n    private oneMinusRho;\r\n    private accumulatedGrads;\r\n    private accumulatedUpdates;\r\n    constructor(learningRate: number, rho: number, epsilon?: number);\r\n    applyGradients(variableGradients: NamedVariableMap): void;\r\n    dispose(): void;\r\n    getConfig(): ConfigDict;\r\n    static fromConfig<T extends Serializable>(cls: SerializableConstructor<T>, config: ConfigDict): T;\r\n}\r\n"}}
