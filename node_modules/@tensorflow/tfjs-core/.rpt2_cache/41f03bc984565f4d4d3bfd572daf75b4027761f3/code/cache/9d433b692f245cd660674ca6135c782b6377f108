{"code":"import * as tslib_1 from \"tslib\";\r\nimport { ENV } from '../environment';\r\nimport { keep, tidy } from '../globals';\r\nimport { scalar, zerosLike } from '../ops/ops';\r\nimport { SerializationMap } from '../serialization';\r\nimport { Optimizer } from './optimizer';\r\nvar AdamaxOptimizer = (function (_super) {\r\n    tslib_1.__extends(AdamaxOptimizer, _super);\r\n    function AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay) {\r\n        if (epsilon === void 0) { epsilon = 1e-8; }\r\n        if (decay === void 0) { decay = 0.0; }\r\n        var _this = _super.call(this) || this;\r\n        _this.learningRate = learningRate;\r\n        _this.beta1 = beta1;\r\n        _this.beta2 = beta2;\r\n        _this.epsilon = epsilon;\r\n        _this.decay = decay;\r\n        _this.accumulatedFirstMoment = {};\r\n        _this.accumulatedWeightedInfNorm = {};\r\n        _this.c = keep(scalar(-learningRate));\r\n        _this.epsScalar = keep(scalar(epsilon));\r\n        _this.beta1Scalar = keep(scalar(beta1));\r\n        _this.beta2Scalar = keep(scalar(beta2));\r\n        _this.decayScalar = keep(scalar(decay));\r\n        tidy(function () {\r\n            _this.iteration = scalar(0).variable();\r\n            _this.accBeta1 = scalar(beta1).variable();\r\n        });\r\n        _this.oneMinusBeta1 = keep(scalar(1 - beta1));\r\n        _this.one = keep(scalar(1));\r\n        return _this;\r\n    }\r\n    AdamaxOptimizer.prototype.applyGradients = function (variableGradients) {\r\n        var _this = this;\r\n        tidy(function () {\r\n            var oneMinusAccBeta1 = _this.one.sub(_this.accBeta1);\r\n            var lr = _this.c.div(_this.one.add(_this.decayScalar.mul(_this.iteration)));\r\n            for (var variableName in variableGradients) {\r\n                var value = ENV.engine.registeredVariables[variableName];\r\n                if (_this.accumulatedFirstMoment[variableName] == null) {\r\n                    var trainable = false;\r\n                    _this.accumulatedFirstMoment[variableName] =\r\n                        zerosLike(value).variable(trainable);\r\n                }\r\n                if (_this.accumulatedWeightedInfNorm[variableName] == null) {\r\n                    var trainable = false;\r\n                    _this.accumulatedWeightedInfNorm[variableName] =\r\n                        zerosLike(value).variable(trainable);\r\n                }\r\n                var gradient = variableGradients[variableName];\r\n                var firstMoment = _this.accumulatedFirstMoment[variableName];\r\n                var weightedInfNorm = _this.accumulatedWeightedInfNorm[variableName];\r\n                var newFirstMoment = _this.beta1Scalar.mul(firstMoment)\r\n                    .add(_this.oneMinusBeta1.mul(gradient));\r\n                var ut0 = _this.beta2Scalar.mul(weightedInfNorm);\r\n                var ut1 = gradient.abs();\r\n                var newWeightedInfNorm = ut0.maximum(ut1);\r\n                _this.accumulatedFirstMoment[variableName].assign(newFirstMoment);\r\n                _this.accumulatedWeightedInfNorm[variableName].assign(newWeightedInfNorm);\r\n                var newValue = lr.div(oneMinusAccBeta1)\r\n                    .mul(newFirstMoment.div(_this.epsScalar.add(newWeightedInfNorm)))\r\n                    .add(value);\r\n                value.assign(newValue);\r\n            }\r\n            _this.iteration.assign(_this.iteration.add(_this.one));\r\n            _this.accBeta1.assign(_this.accBeta1.mul(_this.beta1Scalar));\r\n        });\r\n    };\r\n    AdamaxOptimizer.prototype.dispose = function () {\r\n        var _this = this;\r\n        this.c.dispose();\r\n        this.epsScalar.dispose();\r\n        this.accBeta1.dispose();\r\n        this.beta1Scalar.dispose();\r\n        this.beta2Scalar.dispose();\r\n        this.oneMinusBeta1.dispose();\r\n        this.decayScalar.dispose();\r\n        this.iteration.dispose();\r\n        this.one.dispose();\r\n        if (this.accumulatedFirstMoment != null) {\r\n            Object.keys(this.accumulatedFirstMoment)\r\n                .forEach(function (name) { return _this.accumulatedFirstMoment[name].dispose(); });\r\n        }\r\n        if (this.accumulatedWeightedInfNorm != null) {\r\n            Object.keys(this.accumulatedWeightedInfNorm)\r\n                .forEach(function (name) { return _this.accumulatedWeightedInfNorm[name].dispose(); });\r\n        }\r\n    };\r\n    AdamaxOptimizer.prototype.getConfig = function () {\r\n        return {\r\n            learningRate: this.learningRate,\r\n            beta1: this.beta1,\r\n            beta2: this.beta2,\r\n            epsilon: this.epsilon,\r\n            decay: this.decay\r\n        };\r\n    };\r\n    AdamaxOptimizer.fromConfig = function (cls, config) {\r\n        return new cls(config.learningRate, config.beta1, config.beta2, config.epsilon, config.decay);\r\n    };\r\n    AdamaxOptimizer.className = 'AdamaxOptimizer';\r\n    return AdamaxOptimizer;\r\n}(Optimizer));\r\nexport { AdamaxOptimizer };\r\nSerializationMap.register(AdamaxOptimizer);\r\n//# sourceMappingURL=adamax_optimizer.js.map","map":"{\"version\":3,\"file\":\"adamax_optimizer.js\",\"sourceRoot\":\"\",\"sources\":[\"../src/optimizers/adamax_optimizer.ts\"],\"names\":[],\"mappings\":\";AAiBA,OAAO,EAAC,GAAG,EAAC,MAAM,gBAAgB,CAAC;AACnC,OAAO,EAAC,IAAI,EAAE,IAAI,EAAC,MAAM,YAAY,CAAC;AACtC,OAAO,EAAC,MAAM,EAAE,SAAS,EAAC,MAAM,YAAY,CAAC;AAE7C,OAAO,EAAoD,gBAAgB,EAAC,MAAM,kBAAkB,CAAC;AAIrG,OAAO,EAAC,SAAS,EAAC,MAAM,aAAa,CAAC;AAEtC;IAAqC,2CAAS;IAe5C,yBACc,YAAoB,EAAY,KAAa,EAC7C,KAAa,EAAY,OAAc,EACvC,KAAW;QADc,wBAAA,EAAA,cAAc;QACvC,sBAAA,EAAA,WAAW;QAHzB,YAIE,iBAAO,SAgBR;QAnBa,kBAAY,GAAZ,YAAY,CAAQ;QAAY,WAAK,GAAL,KAAK,CAAQ;QAC7C,WAAK,GAAL,KAAK,CAAQ;QAAY,aAAO,GAAP,OAAO,CAAO;QACvC,WAAK,GAAL,KAAK,CAAM;QANjB,4BAAsB,GAAqB,EAAE,CAAC;QAC9C,gCAA0B,GAAqB,EAAE,CAAC;QAOxD,KAAI,CAAC,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC;QACrC,KAAI,CAAC,SAAS,GAAG,IAAI,CAAC,MAAM,CAAC,OAAO,CAAC,CAAC,CAAC;QAEvC,KAAI,CAAC,WAAW,GAAG,IAAI,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC;QACvC,KAAI,CAAC,WAAW,GAAG,IAAI,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC;QAEvC,KAAI,CAAC,WAAW,GAAG,IAAI,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC;QAEvC,IAAI,CAAC;YACH,KAAI,CAAC,SAAS,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,QAAQ,EAAE,CAAC;YACtC,KAAI,CAAC,QAAQ,GAAG,MAAM,CAAC,KAAK,CAAC,CAAC,QAAQ,EAAE,CAAC;QAC3C,CAAC,CAAC,CAAC;QAEH,KAAI,CAAC,aAAa,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,GAAG,KAAK,CAAC,CAAC,CAAC;QAC7C,KAAI,CAAC,GAAG,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;;IAC7B,CAAC;IAED,wCAAc,GAAd,UAAe,iBAAmC;QAAlD,iBA6CC;QA5CC,IAAI,CAAC;YACH,IAAM,gBAAgB,GAAG,KAAI,CAAC,GAAG,CAAC,GAAG,CAAC,KAAI,CAAC,QAAQ,CAAC,CAAC;YACrD,IAAM,EAAE,GAAG,KAAI,CAAC,CAAC,CAAC,GAAG,CAAC,KAAI,CAAC,GAAG,CAAC,GAAG,CAAC,KAAI,CAAC,WAAW,CAAC,GAAG,CAAC,KAAI,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC;YAE1E,GAAG,CAAC,CAAC,IAAM,YAAY,IAAI,iBAAiB,CAAC,CAAC,CAAC;gBAC7C,IAAM,KAAK,GAAG,GAAG,CAAC,MAAM,CAAC,mBAAmB,CAAC,YAAY,CAAC,CAAC;gBAC3D,EAAE,CAAC,CAAC,KAAI,CAAC,sBAAsB,CAAC,YAAY,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;oBACtD,IAAM,SAAS,GAAG,KAAK,CAAC;oBACxB,KAAI,CAAC,sBAAsB,CAAC,YAAY,CAAC;wBACrC,SAAS,CAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;gBAC3C,CAAC;gBACD,EAAE,CAAC,CAAC,KAAI,CAAC,0BAA0B,CAAC,YAAY,CAAC,IAAI,IAAI,CAAC,CAAC,CAAC;oBAC1D,IAAM,SAAS,GAAG,KAAK,CAAC;oBACxB,KAAI,CAAC,0BAA0B,CAAC,YAAY,CAAC;wBACzC,SAAS,CAAC,KAAK,CAAC,CAAC,QAAQ,CAAC,SAAS,CAAC,CAAC;gBAC3C,CAAC;gBAED,IAAM,QAAQ,GAAG,iBAAiB,CAAC,YAAY,CAAC,CAAC;gBACjD,IAAM,WAAW,GAAG,KAAI,CAAC,sBAAsB,CAAC,YAAY,CAAC,CAAC;gBAC9D,IAAM,eAAe,GAAG,KAAI,CAAC,0BAA0B,CAAC,YAAY,CAAC,CAAC;gBAEtE,IAAM,cAAc,GAAG,KAAI,CAAC,WAAW,CAAC,GAAG,CAAC,WAAW,CAAC;qBAC5B,GAAG,CAAC,KAAI,CAAC,aAAa,CAAC,GAAG,CAAC,QAAQ,CAAC,CAAC,CAAC;gBAElE,IAAM,GAAG,GAAG,KAAI,CAAC,WAAW,CAAC,GAAG,CAAC,eAAe,CAAC,CAAC;gBAClD,IAAM,GAAG,GAAG,QAAQ,CAAC,GAAG,EAAE,CAAC;gBAE3B,IAAM,kBAAkB,GAAG,GAAG,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC;gBAE5C,KAAI,CAAC,sBAAsB,CAAC,YAAY,CAAC,CAAC,MAAM,CAAC,cAAc,CAAC,CAAC;gBACjE,KAAI,CAAC,0BAA0B,CAAC,YAAY,CAAC,CAAC,MAAM,CAChD,kBAAkB,CAAC,CAAC;gBAExB,IAAM,QAAQ,GACV,EAAE,CAAC,GAAG,CAAC,gBAAgB,CAAC;qBACnB,GAAG,CAAC,cAAc,CAAC,GAAG,CAAC,KAAI,CAAC,SAAS,CAAC,GAAG,CAAC,kBAAkB,CAAC,CAAC,CAAC;qBAC/D,GAAG,CAAC,KAAK,CAAC,CAAC;gBAEpB,KAAK,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC;YACzB,CAAC;YAED,KAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAI,CAAC,SAAS,CAAC,GAAG,CAAC,KAAI,CAAC,GAAG,CAAC,CAAC,CAAC;YACpD,KAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,KAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,KAAI,CAAC,WAAW,CAAC,CAAC,CAAC;QAC5D,CAAC,CAAC,CAAC;IACL,CAAC;IAED,iCAAO,GAAP;QAAA,iBAsBC;QArBC,IAAI,CAAC,CAAC,CAAC,OAAO,EAAE,CAAC;QACjB,IAAI,CAAC,SAAS,CAAC,OAAO,EAAE,CAAC;QACzB,IAAI,CAAC,QAAQ,CAAC,OAAO,EAAE,CAAC;QACxB,IAAI,CAAC,WAAW,CAAC,OAAO,EAAE,CAAC;QAC3B,IAAI,CAAC,WAAW,CAAC,OAAO,EAAE,CAAC;QAC3B,IAAI,CAAC,aAAa,CAAC,OAAO,EAAE,CAAC;QAE7B,IAAI,CAAC,WAAW,CAAC,OAAO,EAAE,CAAC;QAC3B,IAAI,CAAC,SAAS,CAAC,OAAO,EAAE,CAAC;QAEzB,IAAI,CAAC,GAAG,CAAC,OAAO,EAAE,CAAC;QAEnB,EAAE,CAAC,CAAC,IAAI,CAAC,sBAAsB,IAAI,IAAI,CAAC,CAAC,CAAC;YACxC,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,sBAAsB,CAAC;iBACnC,OAAO,CAAC,UAAA,IAAI,IAAI,OAAA,KAAI,CAAC,sBAAsB,CAAC,IAAI,CAAC,CAAC,OAAO,EAAE,EAA3C,CAA2C,CAAC,CAAC;QACpE,CAAC;QAED,EAAE,CAAC,CAAC,IAAI,CAAC,0BAA0B,IAAI,IAAI,CAAC,CAAC,CAAC;YAC5C,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,0BAA0B,CAAC;iBACvC,OAAO,CAAC,UAAA,IAAI,IAAI,OAAA,KAAI,CAAC,0BAA0B,CAAC,IAAI,CAAC,CAAC,OAAO,EAAE,EAA/C,CAA+C,CAAC,CAAC;QACxE,CAAC;IACH,CAAC;IACD,mCAAS,GAAT;QACE,MAAM,CAAC;YACL,YAAY,EAAE,IAAI,CAAC,YAAY;YAC/B,KAAK,EAAE,IAAI,CAAC,KAAK;YACjB,KAAK,EAAE,IAAI,CAAC,KAAK;YACjB,OAAO,EAAE,IAAI,CAAC,OAAO;YACrB,KAAK,EAAE,IAAI,CAAC,KAAK;SAClB,CAAC;IACJ,CAAC;IACM,0BAAU,GAAjB,UACI,GAA+B,EAAE,MAAkB;QACrD,MAAM,CAAC,IAAI,GAAG,CACV,MAAM,CAAC,YAAY,EAAE,MAAM,CAAC,KAAK,EAAE,MAAM,CAAC,KAAK,EAAE,MAAM,CAAC,OAAO,EAC/D,MAAM,CAAC,KAAK,CAAC,CAAC;IACpB,CAAC;IAxHM,yBAAS,GAAG,iBAAiB,CAAC;IAyHvC,sBAAC;CAAA,AA1HD,CAAqC,SAAS,GA0H7C;SA1HY,eAAe;AA2H5B,gBAAgB,CAAC,QAAQ,CAAC,eAAe,CAAC,CAAC\"}","dts":{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/rollup/tfjs-core/optimizers/adamax_optimizer.d.ts","text":"import { ConfigDict, Serializable, SerializableConstructor } from '../serialization';\r\nimport { NamedVariableMap } from '../types';\r\nimport { Optimizer } from './optimizer';\r\nexport declare class AdamaxOptimizer extends Optimizer {\r\n    protected learningRate: number;\r\n    protected beta1: number;\r\n    protected beta2: number;\r\n    protected epsilon: number;\r\n    protected decay: number;\r\n    static className: string;\r\n    private c;\r\n    private epsScalar;\r\n    private accBeta1;\r\n    private beta1Scalar;\r\n    private beta2Scalar;\r\n    private decayScalar;\r\n    private oneMinusBeta1;\r\n    private one;\r\n    private iteration;\r\n    private accumulatedFirstMoment;\r\n    private accumulatedWeightedInfNorm;\r\n    constructor(learningRate: number, beta1: number, beta2: number, epsilon?: number, decay?: number);\r\n    applyGradients(variableGradients: NamedVariableMap): void;\r\n    dispose(): void;\r\n    getConfig(): ConfigDict;\r\n    static fromConfig<T extends Serializable>(cls: SerializableConstructor<T>, config: ConfigDict): T;\r\n}\r\n"}}
