{"code":"import * as tslib_1 from \"tslib\";\r\nimport { doc } from '../doc';\r\nimport { AdadeltaOptimizer } from './adadelta_optimizer';\r\nimport { AdagradOptimizer } from './adagrad_optimizer';\r\nimport { AdamOptimizer } from './adam_optimizer';\r\nimport { AdamaxOptimizer } from './adamax_optimizer';\r\nimport { MomentumOptimizer } from './momentum_optimizer';\r\nimport { RMSPropOptimizer } from './rmsprop_optimizer';\r\nimport { SGDOptimizer } from './sgd_optimizer';\r\nvar OptimizerConstructors = (function () {\r\n    function OptimizerConstructors() {\r\n    }\r\n    OptimizerConstructors.sgd = function (learningRate) {\r\n        return new SGDOptimizer(learningRate);\r\n    };\r\n    OptimizerConstructors.momentum = function (learningRate, momentum, useNesterov) {\r\n        if (useNesterov === void 0) { useNesterov = false; }\r\n        return new MomentumOptimizer(learningRate, momentum, useNesterov);\r\n    };\r\n    OptimizerConstructors.rmsprop = function (learningRate, decay, momentum, epsilon, centered) {\r\n        if (decay === void 0) { decay = .9; }\r\n        if (momentum === void 0) { momentum = 0.0; }\r\n        if (epsilon === void 0) { epsilon = 1e-8; }\r\n        if (centered === void 0) { centered = false; }\r\n        return new RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\r\n    };\r\n    OptimizerConstructors.adam = function (learningRate, beta1, beta2, epsilon) {\r\n        if (learningRate === void 0) { learningRate = 0.001; }\r\n        if (beta1 === void 0) { beta1 = 0.9; }\r\n        if (beta2 === void 0) { beta2 = 0.999; }\r\n        if (epsilon === void 0) { epsilon = 1e-8; }\r\n        return new AdamOptimizer(learningRate, beta1, beta2, epsilon);\r\n    };\r\n    OptimizerConstructors.adadelta = function (learningRate, rho, epsilon) {\r\n        if (learningRate === void 0) { learningRate = .001; }\r\n        if (rho === void 0) { rho = .95; }\r\n        if (epsilon === void 0) { epsilon = 1e-8; }\r\n        return new AdadeltaOptimizer(learningRate, rho, epsilon);\r\n    };\r\n    OptimizerConstructors.adamax = function (learningRate, beta1, beta2, epsilon, decay) {\r\n        if (learningRate === void 0) { learningRate = 0.002; }\r\n        if (beta1 === void 0) { beta1 = 0.9; }\r\n        if (beta2 === void 0) { beta2 = 0.999; }\r\n        if (epsilon === void 0) { epsilon = 1e-8; }\r\n        if (decay === void 0) { decay = 0.0; }\r\n        return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\r\n    };\r\n    OptimizerConstructors.adagrad = function (learningRate, initialAccumulatorValue) {\r\n        if (initialAccumulatorValue === void 0) { initialAccumulatorValue = 0.1; }\r\n        return new AdagradOptimizer(learningRate, initialAccumulatorValue);\r\n    };\r\n    tslib_1.__decorate([\r\n        doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\r\n    ], OptimizerConstructors, \"sgd\", null);\r\n    tslib_1.__decorate([\r\n        doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\r\n    ], OptimizerConstructors, \"momentum\", null);\r\n    tslib_1.__decorate([\r\n        doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\r\n    ], OptimizerConstructors, \"rmsprop\", null);\r\n    tslib_1.__decorate([\r\n        doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\r\n    ], OptimizerConstructors, \"adam\", null);\r\n    tslib_1.__decorate([\r\n        doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\r\n    ], OptimizerConstructors, \"adadelta\", null);\r\n    tslib_1.__decorate([\r\n        doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\r\n    ], OptimizerConstructors, \"adamax\", null);\r\n    tslib_1.__decorate([\r\n        doc({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })\r\n    ], OptimizerConstructors, \"adagrad\", null);\r\n    return OptimizerConstructors;\r\n}());\r\nexport { OptimizerConstructors };\r\n//# sourceMappingURL=optimizer_constructors.js.map","map":"{\"version\":3,\"file\":\"optimizer_constructors.js\",\"sourceRoot\":\"\",\"sources\":[\"../src/optimizers/optimizer_constructors.ts\"],\"names\":[],\"mappings\":\";AAiBA,OAAO,EAAC,GAAG,EAAC,MAAM,QAAQ,CAAC;AAE3B,OAAO,EAAC,iBAAiB,EAAC,MAAM,sBAAsB,CAAC;AACvD,OAAO,EAAC,gBAAgB,EAAC,MAAM,qBAAqB,CAAC;AACrD,OAAO,EAAC,aAAa,EAAC,MAAM,kBAAkB,CAAC;AAC/C,OAAO,EAAC,eAAe,EAAC,MAAM,oBAAoB,CAAC;AACnD,OAAO,EAAC,iBAAiB,EAAC,MAAM,sBAAsB,CAAC;AACvD,OAAO,EAAC,gBAAgB,EAAC,MAAM,qBAAqB,CAAC;AACrD,OAAO,EAAC,YAAY,EAAC,MAAM,iBAAiB,CAAC;AAE7C;IAAA;IA4JA,CAAC;IAvHQ,yBAAG,GAAV,UAAW,YAAoB;QAC7B,MAAM,CAAC,IAAI,YAAY,CAAC,YAAY,CAAC,CAAC;IACxC,CAAC;IAgBM,8BAAQ,GAAf,UAAgB,YAAoB,EAAE,QAAgB,EAAE,WAAmB;QAAnB,4BAAA,EAAA,mBAAmB;QAEzE,MAAM,CAAC,IAAI,iBAAiB,CAAC,YAAY,EAAE,QAAQ,EAAE,WAAW,CAAC,CAAC;IACpE,CAAC;IAqBM,6BAAO,GAAd,UACI,YAAoB,EAAE,KAAU,EAAE,QAAc,EAAE,OAAc,EAChE,QAAgB;QADM,sBAAA,EAAA,UAAU;QAAE,yBAAA,EAAA,cAAc;QAAE,wBAAA,EAAA,cAAc;QAChE,yBAAA,EAAA,gBAAgB;QAElB,MAAM,CAAC,IAAI,gBAAgB,CAAC,YAAY,EAAE,KAAK,EAAE,QAAQ,EAAE,OAAO,EAChE,QAAQ,CAAC,CAAC;IACd,CAAC;IAaM,0BAAI,GAAX,UAAY,YAAoB,EAAE,KAAW,EAAE,KAAa,EAAE,OAAc;QAAhE,6BAAA,EAAA,oBAAoB;QAAE,sBAAA,EAAA,WAAW;QAAE,sBAAA,EAAA,aAAa;QAAE,wBAAA,EAAA,cAAc;QAE1E,MAAM,CAAC,IAAI,aAAa,CAAC,YAAY,EAAE,KAAK,EAAE,KAAK,EAAE,OAAO,CAAC,CAAC;IAChE,CAAC;IAaM,8BAAQ,GAAf,UAAgB,YAAmB,EAAE,GAAS,EAAE,OAAc;QAA9C,6BAAA,EAAA,mBAAmB;QAAE,oBAAA,EAAA,SAAS;QAAE,wBAAA,EAAA,cAAc;QAE5D,MAAM,CAAC,IAAI,iBAAiB,CAAC,YAAY,EAAE,GAAG,EAAE,OAAO,CAAC,CAAC;IAC3D,CAAC;IAcM,4BAAM,GAAb,UACI,YAAoB,EAAE,KAAW,EAAE,KAAa,EAAE,OAAc,EAChE,KAAW;QADX,6BAAA,EAAA,oBAAoB;QAAE,sBAAA,EAAA,WAAW;QAAE,sBAAA,EAAA,aAAa;QAAE,wBAAA,EAAA,cAAc;QAChE,sBAAA,EAAA,WAAW;QACb,MAAM,CAAC,IAAI,eAAe,CAAC,YAAY,EAAE,KAAK,EAAE,KAAK,EAAE,OAAO,EAAE,KAAK,CAAC,CAAC;IACzE,CAAC;IAiBM,6BAAO,GAAd,UAAe,YAAoB,EAAE,uBAA6B;QAA7B,wCAAA,EAAA,6BAA6B;QAEhE,MAAM,CAAC,IAAI,gBAAgB,CAAC,YAAY,EAAE,uBAAuB,CAAC,CAAC;IACrE,CAAC;IAtHD;QADC,GAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;0CAGxE;IAgBD;QADC,GAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;+CAIxE;IAqBD;QADC,GAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;8CAOxE;IAaD;QADC,GAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;2CAIxE;IAaD;QADC,GAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;+CAIxE;IAcD;QADC,GAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;6CAKxE;IAiBD;QADC,GAAG,CAAC,EAAC,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,YAAY,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC;8CAIxE;IACH,4BAAC;CAAA,AA5JD,IA4JC;SA5JY,qBAAqB\"}","dts":{"name":"/usr/local/google/home/nsthorat/deeplearnjs-clients/rollup/tfjs-core/optimizers/optimizer_constructors.d.ts","text":"import { AdadeltaOptimizer } from './adadelta_optimizer';\r\nimport { AdagradOptimizer } from './adagrad_optimizer';\r\nimport { AdamOptimizer } from './adam_optimizer';\r\nimport { AdamaxOptimizer } from './adamax_optimizer';\r\nimport { MomentumOptimizer } from './momentum_optimizer';\r\nimport { RMSPropOptimizer } from './rmsprop_optimizer';\r\nimport { SGDOptimizer } from './sgd_optimizer';\r\nexport declare class OptimizerConstructors {\r\n    static sgd(learningRate: number): SGDOptimizer;\r\n    static momentum(learningRate: number, momentum: number, useNesterov?: boolean): MomentumOptimizer;\r\n    static rmsprop(learningRate: number, decay?: number, momentum?: number, epsilon?: number, centered?: boolean): RMSPropOptimizer;\r\n    static adam(learningRate?: number, beta1?: number, beta2?: number, epsilon?: number): AdamOptimizer;\r\n    static adadelta(learningRate?: number, rho?: number, epsilon?: number): AdadeltaOptimizer;\r\n    static adamax(learningRate?: number, beta1?: number, beta2?: number, epsilon?: number, decay?: number): AdamaxOptimizer;\r\n    static adagrad(learningRate: number, initialAccumulatorValue?: number): AdagradOptimizer;\r\n}\r\n"}}
